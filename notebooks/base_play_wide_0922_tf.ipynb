{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "# free some space\n",
    "del train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the data, change to -1,1\n",
    "X_train = X_train / 127.5 - 1\n",
    "test = test / 127.5 -1\n",
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38220, 28, 28, 1) (38220, 10)\n"
     ]
    }
   ],
   "source": [
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "\n",
    "# set random seed for network, maybe not useful\n",
    "from numpy.random import seed \n",
    "seed(42)\n",
    "\n",
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.12, random_state=2)\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile done\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# conv1\n",
    "model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# conv3\n",
    "model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# flatten\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = \"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "# Compile the model\n",
    "\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer = opt , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(\"compile done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jac/.local/lib/python3.5/site-packages/keras/preprocessing/image.py:653: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (38220, 28, 28, 1) (1 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        ) \n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "Epoch 00000: val_loss improved from inf to 0.15086, saving model to wide_best_m_v2.h5\n",
      "7s - loss: 0.9409 - acc: 0.6739 - val_loss: 0.1509 - val_acc: 0.9577\n",
      "Epoch 2/60\n",
      "Epoch 00001: val_loss improved from 0.15086 to 0.08461, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.2561 - acc: 0.9239 - val_loss: 0.0846 - val_acc: 0.9738\n",
      "Epoch 3/60\n",
      "Epoch 00002: val_loss improved from 0.08461 to 0.07541, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.1704 - acc: 0.9458 - val_loss: 0.0754 - val_acc: 0.9772\n",
      "Epoch 4/60\n",
      "Epoch 00003: val_loss improved from 0.07541 to 0.05996, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.1267 - acc: 0.9627 - val_loss: 0.0600 - val_acc: 0.9791\n",
      "Epoch 5/60\n",
      "Epoch 00004: val_loss did not improve\n",
      "6s - loss: 0.1224 - acc: 0.9625 - val_loss: 0.0602 - val_acc: 0.9796\n",
      "Epoch 6/60\n",
      "Epoch 00005: val_loss improved from 0.05996 to 0.05210, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.1186 - acc: 0.9656 - val_loss: 0.0521 - val_acc: 0.9849\n",
      "Epoch 7/60\n",
      "Epoch 00006: val_loss improved from 0.05210 to 0.04777, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0946 - acc: 0.9731 - val_loss: 0.0478 - val_acc: 0.9847\n",
      "Epoch 8/60\n",
      "Epoch 00007: val_loss did not improve\n",
      "6s - loss: 0.0917 - acc: 0.9727 - val_loss: 0.0552 - val_acc: 0.9844\n",
      "Epoch 9/60\n",
      "Epoch 00008: val_loss did not improve\n",
      "6s - loss: 0.0828 - acc: 0.9756 - val_loss: 0.0615 - val_acc: 0.9833\n",
      "Epoch 10/60\n",
      "Epoch 00009: val_loss improved from 0.04777 to 0.04654, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0762 - acc: 0.9766 - val_loss: 0.0465 - val_acc: 0.9852\n",
      "Epoch 11/60\n",
      "Epoch 00010: val_loss improved from 0.04654 to 0.03146, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.1001 - acc: 0.9720 - val_loss: 0.0315 - val_acc: 0.9899\n",
      "Epoch 12/60\n",
      "Epoch 00011: val_loss did not improve\n",
      "6s - loss: 0.0867 - acc: 0.9721 - val_loss: 0.0405 - val_acc: 0.9870\n",
      "Epoch 13/60\n",
      "Epoch 00012: val_loss did not improve\n",
      "6s - loss: 0.0630 - acc: 0.9806 - val_loss: 0.0394 - val_acc: 0.9881\n",
      "Epoch 14/60\n",
      "Epoch 00013: val_loss improved from 0.03146 to 0.02483, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0745 - acc: 0.9767 - val_loss: 0.0248 - val_acc: 0.9926\n",
      "Epoch 15/60\n",
      "Epoch 00014: val_loss did not improve\n",
      "6s - loss: 0.0790 - acc: 0.9775 - val_loss: 0.0351 - val_acc: 0.9907\n",
      "Epoch 16/60\n",
      "Epoch 00015: val_loss did not improve\n",
      "6s - loss: 0.0717 - acc: 0.9792 - val_loss: 0.0351 - val_acc: 0.9892\n",
      "Epoch 17/60\n",
      "Epoch 00016: val_loss did not improve\n",
      "6s - loss: 0.0669 - acc: 0.9808 - val_loss: 0.0318 - val_acc: 0.9905\n",
      "Epoch 18/60\n",
      "Epoch 00017: val_loss did not improve\n",
      "6s - loss: 0.0699 - acc: 0.9797 - val_loss: 0.0334 - val_acc: 0.9905\n",
      "Epoch 19/60\n",
      "Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 0.0006000000284984708.\n",
      "6s - loss: 0.0640 - acc: 0.9814 - val_loss: 0.0340 - val_acc: 0.9915\n",
      "Epoch 20/60\n",
      "Epoch 00019: val_loss did not improve\n",
      "6s - loss: 0.0429 - acc: 0.9870 - val_loss: 0.0257 - val_acc: 0.9934\n",
      "Epoch 21/60\n",
      "Epoch 00020: val_loss improved from 0.02483 to 0.02280, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0496 - acc: 0.9862 - val_loss: 0.0228 - val_acc: 0.9926\n",
      "Epoch 22/60\n",
      "Epoch 00021: val_loss did not improve\n",
      "6s - loss: 0.0411 - acc: 0.9881 - val_loss: 0.0337 - val_acc: 0.9913\n",
      "Epoch 23/60\n",
      "Epoch 00022: val_loss improved from 0.02280 to 0.01788, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0470 - acc: 0.9862 - val_loss: 0.0179 - val_acc: 0.9942\n",
      "Epoch 24/60\n",
      "Epoch 00023: val_loss did not improve\n",
      "6s - loss: 0.0425 - acc: 0.9875 - val_loss: 0.0348 - val_acc: 0.9905\n",
      "Epoch 25/60\n",
      "Epoch 00024: val_loss did not improve\n",
      "6s - loss: 0.0396 - acc: 0.9889 - val_loss: 0.0225 - val_acc: 0.9931\n",
      "Epoch 26/60\n",
      "Epoch 00025: val_loss improved from 0.01788 to 0.01737, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0395 - acc: 0.9883 - val_loss: 0.0174 - val_acc: 0.9931\n",
      "Epoch 27/60\n",
      "Epoch 00026: val_loss improved from 0.01737 to 0.01669, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0408 - acc: 0.9887 - val_loss: 0.0167 - val_acc: 0.9950\n",
      "Epoch 28/60\n",
      "Epoch 00027: val_loss improved from 0.01669 to 0.01627, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0459 - acc: 0.9883 - val_loss: 0.0163 - val_acc: 0.9939\n",
      "Epoch 29/60\n",
      "Epoch 00028: val_loss did not improve\n",
      "6s - loss: 0.0485 - acc: 0.9848 - val_loss: 0.0229 - val_acc: 0.9929\n",
      "Epoch 30/60\n",
      "Epoch 00029: val_loss did not improve\n",
      "6s - loss: 0.0429 - acc: 0.9870 - val_loss: 0.0249 - val_acc: 0.9913\n",
      "Epoch 31/60\n",
      "Epoch 00030: val_loss did not improve\n",
      "6s - loss: 0.0370 - acc: 0.9886 - val_loss: 0.0249 - val_acc: 0.9931\n",
      "Epoch 32/60\n",
      "Epoch 00031: val_loss did not improve\n",
      "\n",
      "Epoch 00031: reducing learning rate to 0.0003600000170990825.\n",
      "6s - loss: 0.0493 - acc: 0.9847 - val_loss: 0.0234 - val_acc: 0.9926\n",
      "Epoch 33/60\n",
      "Epoch 00032: val_loss improved from 0.01627 to 0.01516, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0375 - acc: 0.9897 - val_loss: 0.0152 - val_acc: 0.9950\n",
      "Epoch 34/60\n",
      "Epoch 00033: val_loss did not improve\n",
      "6s - loss: 0.0323 - acc: 0.9891 - val_loss: 0.0190 - val_acc: 0.9942\n",
      "Epoch 35/60\n",
      "Epoch 00034: val_loss improved from 0.01516 to 0.01400, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0303 - acc: 0.9908 - val_loss: 0.0140 - val_acc: 0.9952\n",
      "Epoch 36/60\n",
      "Epoch 00035: val_loss did not improve\n",
      "6s - loss: 0.0313 - acc: 0.9903 - val_loss: 0.0142 - val_acc: 0.9955\n",
      "Epoch 37/60\n",
      "Epoch 00036: val_loss did not improve\n",
      "6s - loss: 0.0246 - acc: 0.9917 - val_loss: 0.0141 - val_acc: 0.9963\n",
      "Epoch 38/60\n",
      "Epoch 00037: val_loss did not improve\n",
      "6s - loss: 0.0319 - acc: 0.9898 - val_loss: 0.0172 - val_acc: 0.9955\n",
      "Epoch 39/60\n",
      "Epoch 00038: val_loss improved from 0.01400 to 0.01378, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0242 - acc: 0.9927 - val_loss: 0.0138 - val_acc: 0.9960\n",
      "Epoch 40/60\n",
      "Epoch 00039: val_loss did not improve\n",
      "6s - loss: 0.0330 - acc: 0.9916 - val_loss: 0.0145 - val_acc: 0.9955\n",
      "Epoch 41/60\n",
      "Epoch 00040: val_loss did not improve\n",
      "6s - loss: 0.0224 - acc: 0.9928 - val_loss: 0.0174 - val_acc: 0.9952\n",
      "Epoch 42/60\n",
      "Epoch 00041: val_loss did not improve\n",
      "\n",
      "Epoch 00041: reducing learning rate to 0.00021600000327453016.\n",
      "6s - loss: 0.0273 - acc: 0.9911 - val_loss: 0.0147 - val_acc: 0.9952\n",
      "Epoch 43/60\n",
      "Epoch 00042: val_loss improved from 0.01378 to 0.01276, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0252 - acc: 0.9919 - val_loss: 0.0128 - val_acc: 0.9960\n",
      "Epoch 44/60\n",
      "Epoch 00043: val_loss did not improve\n",
      "6s - loss: 0.0256 - acc: 0.9930 - val_loss: 0.0149 - val_acc: 0.9952\n",
      "Epoch 45/60\n",
      "Epoch 00044: val_loss did not improve\n",
      "6s - loss: 0.0261 - acc: 0.9920 - val_loss: 0.0134 - val_acc: 0.9963\n",
      "Epoch 46/60\n",
      "Epoch 00045: val_loss did not improve\n",
      "\n",
      "Epoch 00045: reducing learning rate to 0.00012960000021848827.\n",
      "6s - loss: 0.0262 - acc: 0.9928 - val_loss: 0.0134 - val_acc: 0.9958\n",
      "Epoch 47/60\n",
      "Epoch 00046: val_loss did not improve\n",
      "6s - loss: 0.0261 - acc: 0.9931 - val_loss: 0.0137 - val_acc: 0.9960\n",
      "Epoch 48/60\n",
      "Epoch 00047: val_loss did not improve\n",
      "6s - loss: 0.0273 - acc: 0.9920 - val_loss: 0.0150 - val_acc: 0.9952\n",
      "Epoch 49/60\n",
      "Epoch 00048: val_loss improved from 0.01276 to 0.01244, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0194 - acc: 0.9950 - val_loss: 0.0124 - val_acc: 0.9966\n",
      "Epoch 50/60\n",
      "Epoch 00049: val_loss did not improve\n",
      "6s - loss: 0.0235 - acc: 0.9925 - val_loss: 0.0139 - val_acc: 0.9966\n",
      "Epoch 51/60\n",
      "Epoch 00050: val_loss did not improve\n",
      "6s - loss: 0.0196 - acc: 0.9945 - val_loss: 0.0125 - val_acc: 0.9960\n",
      "Epoch 52/60\n",
      "Epoch 00051: val_loss improved from 0.01244 to 0.01104, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0200 - acc: 0.9938 - val_loss: 0.0110 - val_acc: 0.9968\n",
      "Epoch 53/60\n",
      "Epoch 00052: val_loss did not improve\n",
      "6s - loss: 0.0226 - acc: 0.9939 - val_loss: 0.0123 - val_acc: 0.9971\n",
      "Epoch 54/60\n",
      "Epoch 00053: val_loss improved from 0.01104 to 0.01086, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0210 - acc: 0.9934 - val_loss: 0.0109 - val_acc: 0.9974\n",
      "Epoch 55/60\n",
      "Epoch 00054: val_loss did not improve\n",
      "6s - loss: 0.0159 - acc: 0.9950 - val_loss: 0.0138 - val_acc: 0.9960\n",
      "Epoch 56/60\n",
      "Epoch 00055: val_loss improved from 0.01086 to 0.01068, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0199 - acc: 0.9933 - val_loss: 0.0107 - val_acc: 0.9968\n",
      "Epoch 57/60\n",
      "Epoch 00056: val_loss did not improve\n",
      "6s - loss: 0.0227 - acc: 0.9934 - val_loss: 0.0141 - val_acc: 0.9960\n",
      "Epoch 58/60\n",
      "Epoch 00057: val_loss did not improve\n",
      "6s - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0127 - val_acc: 0.9958\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00058: val_loss did not improve\n",
      "\n",
      "Epoch 00058: reducing learning rate to 7.775999838486313e-05.\n",
      "6s - loss: 0.0192 - acc: 0.9931 - val_loss: 0.0123 - val_acc: 0.9960\n",
      "Epoch 60/60\n",
      "Epoch 00059: val_loss improved from 0.01068 to 0.01040, saving model to wide_best_m_v2.h5\n",
      "6s - loss: 0.0170 - acc: 0.9950 - val_loss: 0.0104 - val_acc: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08ddd7f5f8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=4, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.6, \n",
    "                                            min_lr=0.00001)\n",
    "    \n",
    "best_model_path = 'wide_best_m_v2.h5'\n",
    "best_model = ModelCheckpoint(filepath=best_model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "epochs = 100\n",
    "steps_per_epoch = 100\n",
    "batch_size = 64\n",
    "model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 2, steps_per_epoch=steps_per_epoch,\n",
    "                              callbacks=[best_model,learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save done wide_play_cnn_mnist_datagen_2017-09-22T12_03_50_85.csv\n"
     ]
    }
   ],
   "source": [
    "model = load_model(best_model_path)  # try callback model\n",
    "# predict results\n",
    "results = model.predict(test)\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "import arrow\n",
    "ts = str(arrow.now())[:-10].replace(':','_').replace('.',\"_\")\n",
    "res_file = \"wide_play_cnn_mnist_datagen_{}.csv\".format(ts)\n",
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "submission.to_csv(res_file,index=False)\n",
    "print('save done',res_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
